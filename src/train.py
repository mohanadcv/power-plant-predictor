from preprocessor import *

# Improved data splitting with validation set
X_temp, X_test, Y_temp, Y_test = train_test_split(
    X_processed, Y, test_size=config.TEST_SIZE, random_state=config.RANDOM_STATE
)

X_train, X_val, Y_train, Y_val = train_test_split(
    X_temp, Y_temp, test_size=config.VAL_SIZE, random_state=config.RANDOM_STATE
)

print(f"‚Ä¢ Training: {X_train.shape[0]:,} samples (model learning)")
print(f"‚Ä¢ Validation: {X_val.shape[0]:,} samples (hyperparameter tuning)")
print(f"‚Ä¢ Test: {X_test.shape[0]:,} samples (final evaluation - NEVER TOUCHED until end)")


# define advanced models - Expanded from previous work
advanced_models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(random_state=config.RANDOM_STATE),
    'Lasso Regression': Lasso(random_state=config.RANDOM_STATE),
    'ElasticNet': ElasticNet(random_state=config.RANDOM_STATE),  # NEW: Combines L1 + L2
    'Random Forest': RandomForestRegressor(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS),
    'Gradient Boosting': GradientBoostingRegressor(random_state=config.RANDOM_STATE),  # NEW: Sequential learning
    'Support Vector Regression': SVR(),  # NEW: Different approach
}

# NEW: Voting Ensemble - Combines multiple models
voting_ensemble = VotingRegressor([
    ('ridge', Ridge(random_state=config.RANDOM_STATE)),
    ('rf', RandomForestRegressor(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS)),
    ('gb', GradientBoostingRegressor(random_state=config.RANDOM_STATE))
])

advanced_models['Voting Ensemble'] = voting_ensemble

print(f"\nüéØ MODEL PORTFOLIO ({len(advanced_models)} models):")

# --------------------------------------------------------------------

# Advanced model training, evaluation, and logging with mlflow (All-in-One)
def evaluate_model_advanced(model, X_train, X_val, Y_train, Y_val, model_name):
    """Comprehensive model evaluation with MLflow tracking"""

    # Start MLflow run for experiment tracking
    with mlflow.start_run(run_name=model_name):
        # Train model with timing
        start_time = time.time()
        model.fit(X_train, Y_train)
        training_time = time.time() - start_time

        # Predictions on both sets
        Y_train_pred = model.predict(X_train)
        Y_val_pred = model.predict(X_val)

        # Calculate comprehensive metrics
        metrics = {
            'train_rmse': np.sqrt(mean_squared_error(Y_train, Y_train_pred)),
            'val_rmse': np.sqrt(mean_squared_error(Y_val, Y_val_pred)),
            'train_r2': r2_score(Y_train, Y_train_pred),
            'val_r2': r2_score(Y_val, Y_val_pred),
            'train_mae': mean_absolute_error(Y_train, Y_train_pred),
            'val_mae': mean_absolute_error(Y_val, Y_val_pred),
            'training_time': training_time,
            'overfitting_gap': r2_score(Y_train, Y_train_pred) - r2_score(Y_val, Y_val_pred)
        }

        # AUTOMATIC TRACKING with MLflow
        mlflow.log_params(model.get_params())  # Log hyperparameters
        mlflow.log_metrics({k: v for k, v in metrics.items() if k != 'training_time'})
        mlflow.sklearn.log_model(model, "model")  # Save model artifact

        # Cross-validation for robust performance estimate
        cv_scores = cross_val_score(model, X_train, Y_train,
                                  cv=config.CV_FOLDS, scoring='r2', n_jobs=config.N_JOBS)
        metrics['cv_r2_mean'] = cv_scores.mean()
        metrics['cv_r2_std'] = cv_scores.std()

        mlflow.log_metrics({
            'cv_r2_mean': metrics['cv_r2_mean'],
            'cv_r2_std': metrics['cv_r2_std']
        })

        return metrics, model

print("üöÄ STARTING ADVANCED MODEL EVALUATION...")
print("Each model is being trained, evaluated, and tracked in MLflow")

results = {}
trained_models = {}

for name, model in advanced_models.items():
    print(f"\nüîß Training {name}...")
    metrics, trained_model = evaluate_model_advanced(
        model, X_train, X_val, Y_train, Y_val, name
    )
    results[name] = metrics
    trained_models[name] = trained_model

    # Progress reporting
    overfitting_indicator = "‚ö†Ô∏è" if metrics['overfitting_gap'] > 0.1 else "‚úÖ"
    print(f"‚úÖ {name:20} | Val R¬≤: {metrics['val_r2']:.4f} | "
          f"CV R¬≤: {metrics['cv_r2_mean']:.4f} ¬± {metrics['cv_r2_std']:.4f} "
          f"{overfitting_indicator}")

print(f"\nüìà All models trained and tracked in MLflow!")
print(f"üí° Check MLflow UI: mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}")

# --------------------------------------------------------------------

# Advanced Hyperparameter Optimization

# Define comprehensive hyperparameter grids
param_grids = {
    'Random Forest': {
        'n_estimators': [50, 100, 200, 300],  # Number of trees
        'max_depth': [None, 10, 20, 30],  # Tree depth
        'min_samples_split': [2, 5, 10],  # Minimum samples to split
        'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf
        'max_features': ['auto', 'sqrt', 'log2']  # Features to consider for splits
    },

    'Gradient Boosting': {
        'n_estimators': [50, 100, 200],  # Number of boosting stages
        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage
        'max_depth': [3, 4, 5, 6],  # Maximum depth per tree
        'min_samples_split': [2, 5, 10],  # Minimum samples to split
        'subsample': [0.8, 0.9, 1.0]  # Fraction of samples for fitting
    },

    'Ridge Regression': {
        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],  # Regularization strength
        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']  # Algorithm
    },

    'Voting Ensemble': {
        'ridge__alpha': [0.1, 1.0, 10.0],
        'rf__n_estimators': [50, 100],
        'rf__max_depth': [10, 20],
        'gb__n_estimators': [50, 100],
        'gb__learning_rate': [0.05, 0.1]
    }
}


# Perform hyperparameter optimization
print("üéØ STARTING HYPERPARAMETER OPTIMIZATION...")
tuned_models = {}
optimization_results = {}

for model_name in ['Random Forest', 'Gradient Boosting', 'Ridge Regression', 'Voting Ensemble']:
    print(f"\nüîß Tuning {model_name}...")

    with mlflow.start_run(run_name=f"{model_name}_tuned"):
        # Use RandomizedSearchCV for efficient optimization
        search = RandomizedSearchCV(
            advanced_models[model_name],
            param_grids[model_name],
            n_iter=20,  # Try 20 random combinations (efficient!)
            cv=config.CV_FOLDS,
            scoring='r2',
            n_jobs=config.N_JOBS,
            random_state=config.RANDOM_STATE,
            verbose=1
        )

        # Perform the search
        search.fit(X_train, Y_train)

        # Store results
        tuned_models[model_name] = search.best_estimator_
        optimization_results[model_name] = {
            'best_score': search.best_score_,
            'best_params': search.best_params_,
            'best_estimator': search.best_estimator_
        }

        # Log to MLflow
        mlflow.log_params(search.best_params_)
        mlflow.log_metric('best_cv_score', search.best_score_)
        mlflow.sklearn.log_model(search.best_estimator_, "tuned_model")

        print(f"‚úÖ {model_name:20} | Best CV R¬≤: {search.best_score_:.4f}")
        print(f"   Best parameters found: {search.best_params_}")

print(f"\nüéâ HYPERPARAMETER OPTIMIZATION COMPLETE!")
print(f"üí° All tuned models saved in MLflow for comparison")


# ===========================
#  Evaluate tuned models on validation set
# ===========================
tuned_results = {}
for model_name, tuned_model in tuned_models.items():
    y_val_pred = tuned_model.predict(X_val)
    val_r2 = r2_score(Y_val, y_val_pred)
    val_rmse = np.sqrt(mean_squared_error(Y_val, y_val_pred))
    improvement = val_r2 - results[model_name]['val_r2']  # vs untuned

    tuned_results[model_name] = {
        'val_r2': val_r2,
        'val_rmse': val_rmse,
        'improvement': improvement
    }

# --------------------------------------------------------------------

# Model Deployment Preparation
# ===========================
# Prepare feature names
# ===========================
# Works for both DataFrame and NumPy array
try:
    feature_names_all = list(X.columns)
except AttributeError:
    feature_names_all = [f"feature_{i}" for i in range(X.shape[1])]

# ===========================
# Identify best tuned model
# ===========================
best_model_name = max(tuned_results, key=lambda m: tuned_results[m]['val_r2'])
best_tuned_model = tuned_models[best_model_name]

print(f"üèÜ BEST TUNED MODEL: {best_model_name}")
print(f"üìä Validation R¬≤: {tuned_results[best_model_name]['val_r2']:.4f}")
print(f"üìà Improvement over untuned: +{tuned_results[best_model_name]['improvement']:.4f}")

# ===========================
# Evaluate on Test Set
# ===========================
Y_test_pred = best_tuned_model.predict(X_test)
test_r2 = r2_score(Y_test, Y_test_pred)
test_rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))
test_mae = mean_absolute_error(Y_test, Y_test_pred)

print(f"\nüìä Test Set Performance:")
print(f"R¬≤: {test_r2:.4f} | RMSE: {test_rmse:.4f} | MAE: {test_mae:.4f}")

# ===========================
# Versioning and saving
# ===========================
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_version = f"v1_{timestamp}"
model_save_dir = os.path.join(config.MODEL_DIR, model_version)
os.makedirs(model_save_dir, exist_ok=True)

# Save model
model_path = os.path.join(model_save_dir, 'best_model.pkl')
joblib.dump(best_tuned_model, model_path)
print(f"‚úÖ Model saved: {model_path}")

# Save preprocessing pipeline
preprocessor_path = os.path.join(model_save_dir, 'preprocessor.pkl')
joblib.dump(preprocessor, preprocessor_path)
print(f"‚úÖ Preprocessor saved: {preprocessor_path}")

# ===========================
# Create model card
# ===========================
model_card = {
    'model_name': best_model_name,
    'model_version': model_version,
    'timestamp': timestamp,
    'dataset': 'California Housing',
    'target': 'House Price ($M)',

    'performance': {
        'test_r2': float(test_r2),
        'test_rmse': float(test_rmse),
        'test_mae': float(test_mae),
        'train_r2': float(results[best_model_name]['train_r2']),
        'val_r2': float(tuned_results[best_model_name]['val_r2']),
        'cv_r2_mean': float(results[best_model_name]['cv_r2_mean']),
        'cv_r2_std': float(results[best_model_name]['cv_r2_std']),
    },

    'data_info': {
        'total_samples': int(len(X)),
        'train_samples': int(len(X_train)),
        'val_samples': int(len(X_val)),
        'test_samples': int(len(X_test)),
        'n_features': X_test.shape[1],
        'feature_names': feature_names_all,
    },

    'model_config': {
        'model_class': best_model_name,
        'hyperparameters': dict(best_tuned_model.get_params())
        if hasattr(best_tuned_model, 'get_params') else {},
    },

    'preprocessing': {
        'steps': [
            'AdvancedFeatureEngineering (6 new features)',
            'OutlierHandling (IQR method)',
            'RobustScaler (outlier-resistant scaling)',
        ],
        'outlier_factor': 1.5,
    },

    'deployment': {
        'status': 'Ready for Production',
        'recommendations': [
            'Monitor prediction errors in production',
            'Retrain quarterly with new data',
            'Alert if RMSE exceeds $0.50M',
        ]
    }
}

card_path = os.path.join(model_save_dir, 'model_card.json')
with open(card_path, 'w') as f:
    json.dump(model_card, f, indent=2)
print(f"‚úÖ Model card saved: {card_path}")

# ===========================
# 5Ô∏è‚É£ Deployment requirements
# ===========================
requirements = {
    'python': '3.8+',
    'packages': {
        'scikit-learn': '1.0+',
        'numpy': '1.20+',
        'pandas': '1.3+',
        'joblib': '1.0+',
    }
}

req_path = os.path.join(model_save_dir, 'requirements.json')
with open(req_path, 'w') as f:
    json.dump(requirements, f, indent=2)
print(f"‚úÖ Deployment requirements saved: {req_path}")

# ===========================
# Summary
# ===========================
print("\nüíæ DEPLOYMENT PACKAGE READY")
print(f"Location: {model_save_dir}")

joblib.dump(Pipeline, "../best_model.pkl")