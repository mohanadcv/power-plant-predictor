from feature_engineering import *
from train import *

# Distribution Analysis
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for idx, col in enumerate(df.columns):
    axes[idx].hist(df[col], bins=50, color='skyblue', edgecolor='black', alpha=0.7)
    axes[idx].set_title(f'Distribution of ({col})', fontsize=18, fontweight='bold')
    axes[idx].set_xlabel(col, fontsize=18, fontweight='bold')
    axes[idx].set_ylabel('Frequency', fontsize=18, fontweight='bold')
    axes[idx].grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

#--------------------------------------------------------------------

# Relationship Between Target and each Feature
feature_names = ["AT", "V", "AP", "RH",]

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.ravel()
bins = 30                                             # number of bins (controls smoothness)
for i, feature in enumerate(feature_names):
    df["bin"] = pd.cut(df[feature], bins=bins)        # Bin the feature
    grouped = df.groupby("bin")["PE"].mean()          # Mean PE per bin
    x = [interval.mid for interval in grouped.index]  # Bin centers
    y = grouped.values
    axes[i].plot(x, y, linewidth=3)
    axes[i].set_xlabel(feature, fontsize=13, fontweight='bold')
    axes[i].set_ylabel("PE (MW)", fontsize=13, fontweight='bold')
    axes[i].set_title(f"PE vs {feature}", fontsize=13)
    axes[i].grid(True)

plt.tight_layout()
plt.show()

#--------------------------------------------------------------------

# Correlation Matrix
correlation_matrix = df.select_dtypes(include="number").corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()
# Feature importance ranking
print("\n" + "=" * 70)
print("üéØ FEATURES RANKED BY CORRELATION WITH PE")
print("=" * 70)
correlations = correlation_matrix['PE'].drop('PE').sort_values(ascending=False)
for feature, corr in correlations.items():
    print(f"{feature:<15} {corr:>8.4f}")

# --------------------------------------------------------------------

# Relationship Between Target and each Feature including new Features
plot_df = X_engineered.copy()
plot_df[["AT", "V", "AP", "RH"]] = X[["AT", "V", "AP", "RH"]]
plot_df["PE"] = Y

feature_names = ["AT", "V", "AP", "RH",
    "AirDensity", "MoistureMoleFraction", "DryMassFlowIndex"]

fig, axes = plt.subplots(3, 3, figsize=(14, 10))
axes = axes.ravel()
bins = 30

for i, f in enumerate(feature_names):
    grouped = plot_df.groupby(pd.cut(plot_df[f], bins=bins))["PE"].mean()
    x = [b.mid for b in grouped.index]

    axes[i].plot(x, grouped.values, linewidth=3)
    axes[i].set(xlabel=f, ylabel="PE (MW)", title=f"PE vs {f}")
    axes[i].grid(True)

for j in range(len(feature_names), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# --------------------------------------------------------------------

# Correlation Matrix including new Features
feature_names = preprocessor.named_steps['feature_engineer'].get_feature_names()
X_full_df = pd.DataFrame(X_processed, columns=feature_names)
X_full_df['PE'] = Y
corr = X_full_df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title("Feature Correlation with PE (After Engineering & Scaling)")
plt.show()
correlations = corr['PE'].drop('PE').sort_values(ascending=False)
for feature, corr in correlations.items():
    print(f"{feature:<15} {corr:>8.4f}")

# --------------------------------------------------------------------

# Trained models comparison

# ===========================
# 1Ô∏è‚É£ Convert results dict to DataFrame
# ===========================
metrics_df = pd.DataFrame(results).T  # transpose so models are rows
metrics_df = metrics_df[['val_r2', 'val_rmse', 'val_mae', 'overfitting_gap', 'cv_r2_mean']]
metrics_df = metrics_df.reset_index().rename(columns={'index': 'Model'})

print("üìä Model comparison table:")
display(metrics_df)

# ===========================
# 2Ô∏è‚É£ Plot Validation R¬≤ (Higher is better)
# ===========================
plt.figure(figsize=(10, 5))
sns.barplot(data=metrics_df, x='Model', y='val_r2', palette='viridis')
plt.xticks(rotation=45, ha='right')
plt.title('Validation R¬≤ Comparison (Higher is Better ‚úÖ)')
plt.ylabel('Validation R¬≤')
plt.xlabel('')
plt.ylim(0, 1)
plt.show()

# ===========================
# 3Ô∏è‚É£ Plot Validation RMSE (Lower is better)
# ===========================
plt.figure(figsize=(10, 5))
sns.barplot(data=metrics_df, x='Model', y='val_rmse', palette='magma')
plt.xticks(rotation=45, ha='right')
plt.title('Validation RMSE Comparison (Lower is Better ‚úÖ)')
plt.ylabel('Validation RMSE')
plt.xlabel('')
plt.show()

# ===========================
# 4Ô∏è‚É£ Plot Overfitting Gap (Closer to 0 is better)
# ===========================
plt.figure(figsize=(10, 5))
sns.barplot(data=metrics_df, x='Model', y='overfitting_gap', palette='coolwarm')
plt.xticks(rotation=45, ha='right')
plt.title('Overfitting Gap (train R¬≤ - val R¬≤) - Closer to 0 is Better ‚öñÔ∏è')
plt.ylabel('Overfitting Gap')
plt.xlabel('')
plt.show()


# Trained models comparison
print("üìà ADVANCED MODEL COMPARISON")

# Create comprehensive results dataframe
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('val_r2', ascending=False)

print("\n" + "=" * 80)
print("üèÜ MODEL PERFORMANCE RANKING")
print("=" * 80)
print(f"{'Model':<25} {'Val R¬≤':<8} {'CV R¬≤':<12} {'Overfitting':<12} {'Time (s)':<10}")
print("-" * 80)

for model_name in results_df.index:
    row = results_df.loc[model_name]
    overfitting_indicator = "‚ö†Ô∏è" if row['overfitting_gap'] > 0.1 else "‚úÖ"
    print(f"{model_name:<25} {row['val_r2']:>7.4f} {row['cv_r2_mean']:>7.4f} ¬± {row['cv_r2_std']:>5.4f} "
          f"{overfitting_indicator:>3} {row['overfitting_gap']:>7.4f} {row['training_time']:>9.2f}")

# Comprehensive visualization
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. R¬≤ Comparison
models_ordered = results_df.index
val_r2 = [results[model]['val_r2'] for model in models_ordered]
train_r2 = [results[model]['train_r2'] for model in models_ordered]

x = np.arange(len(models_ordered))
width = 0.35

axes[0, 0].bar(x - width/2, train_r2, width, label='Train R¬≤', alpha=0.7, color='skyblue')
axes[0, 0].bar(x + width/2, val_r2, width, label='Val R¬≤', alpha=0.7, color='lightcoral')
axes[0, 0].set_xlabel('Models')
axes[0, 0].set_ylabel('R¬≤ Score')
axes[0, 0].set_title('Training vs Validation Performance\n(Gap indicates overfitting)')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(models_ordered, rotation=45, ha='right')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. Overfitting Analysis
overfitting_gaps = [results[model]['overfitting_gap'] for model in models_ordered]
colors = ['red' if gap > 0.1 else 'green' for gap in overfitting_gaps]
axes[0, 1].bar(models_ordered, overfitting_gaps, color=colors, alpha=0.7)
axes[0, 1].axhline(y=0.1, color='red', linestyle='--', alpha=0.8, label='Overfitting Threshold')
axes[0, 1].set_xlabel('Models')
axes[0, 1].set_ylabel('Overfitting Gap (Train R¬≤ - Val R¬≤)')
axes[0, 1].set_title('Overfitting Detection\n(Red = potential overfitting)')
axes[0, 1].set_xticklabels(models_ordered, rotation=45, ha='right')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. Cross-Validation Stability
cv_means = [results[model]['cv_r2_mean'] for model in models_ordered]
cv_stds = [results[model]['cv_r2_std'] for model in models_ordered]
axes[1, 0].bar(models_ordered, cv_means, yerr=cv_stds, capsize=5, alpha=0.7, color='lightgreen')
axes[1, 0].set_xlabel('Models')
axes[1, 0].set_ylabel('CV R¬≤ Score')
axes[1, 0].set_title('Cross-Validation Performance\n(Error bars = 1 standard deviation)')
axes[1, 0].set_xticklabels(models_ordered, rotation=45, ha='right')
axes[1, 0].grid(True, alpha=0.3)

# 4. Computational Efficiency
training_times = [results[model]['training_time'] for model in models_ordered]
axes[1, 1].bar(models_ordered, training_times, alpha=0.7, color='orange')
axes[1, 1].set_xlabel('Models')
axes[1, 1].set_ylabel('Training Time (seconds)')
axes[1, 1].set_title('Computational Efficiency\n(Important for large datasets)')
axes[1, 1].set_xticklabels(models_ordered, rotation=45, ha='right')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Select best model
best_model_name = results_df.index[0]
best_model = trained_models[best_model_name]
print(f"\nüéØ BEST MODEL SELECTED: {best_model_name}")
print(f"üìä Validation R¬≤: {results_df.iloc[0]['val_r2']:.4f}")
print(f"üîç CV R¬≤: {results_df.iloc[0]['cv_r2_mean']:.4f} ¬± {results_df.iloc[0]['cv_r2_std']:.4f}")

print("\nüí° INTERPRETATION GUIDE:")
print("‚Ä¢ Good: High R¬≤, small train-val gap, stable CV, reasonable training time")
print("‚Ä¢ Overfitting: Large gap between train and validation performance")
print("‚Ä¢ Unstable: Large CV standard deviation")
print("‚Ä¢ Best choice: Balances performance, stability, and efficiency")

# --------------------------------------------------------------------

# Tuned models comparison

# ===========================
# 1Ô∏è‚É£ Convert optimization results to DataFrame
# ===========================
tuned_metrics_df = pd.DataFrame.from_dict({
    model: {
        'Best CV R¬≤': optimization_results[model]['best_score']
    } for model in optimization_results
}, orient='index').reset_index().rename(columns={'index': 'Model'})

print("üìä Tuned Models Performance:")
display(tuned_metrics_df)

# ===========================
# 2Ô∏è‚É£ Plot Best CV R¬≤ (Higher is better)
# ===========================
plt.figure(figsize=(10,5))
sns.barplot(data=tuned_metrics_df, x='Model', y='Best CV R¬≤', palette='viridis')
plt.xticks(rotation=45, ha='right')
plt.title('Tuned Models Best CV R¬≤ Comparison (Higher is Better ‚úÖ)')
plt.ylabel('Best CV R¬≤')
plt.xlabel('')
plt.ylim(0,1)
plt.show()


# Advanced Tuned models comparison

# ===========================
# 1Ô∏è‚É£ Evaluate tuned models on validation set
# ===========================
tuned_results = {}
for model_name, tuned_model in tuned_models.items():
    y_val_pred = tuned_model.predict(X_val)
    val_r2 = r2_score(Y_val, y_val_pred)
    val_rmse = np.sqrt(mean_squared_error(Y_val, y_val_pred))
    improvement = val_r2 - results[model_name]['val_r2']  # vs untuned

    tuned_results[model_name] = {
        'val_r2': val_r2,
        'val_rmse': val_rmse,
        'improvement': improvement
    }

# ===========================
# 2Ô∏è‚É£ Print simple comparison table
# ===========================
print(f"{'Model':<20} {'Untuned R¬≤':<12} {'Tuned R¬≤':<12} {'Improvement':<12}")
print("-" * 60)
for model_name, res in tuned_results.items():
    untuned_r2 = results[model_name]['val_r2']
    tuned_r2 = res['val_r2']
    improvement = res['improvement']
    icon = "üìà" if improvement > 0 else "üìâ" if improvement < 0 else "‚û°Ô∏è"
    print(f"{model_name:<20} {untuned_r2:>10.4f} {tuned_r2:>10.4f} {icon} {improvement:>8.4f}")

# ===========================
# 3Ô∏è‚É£ Plot Tuned vs Untuned R¬≤
# ===========================
models = list(tuned_results.keys())
untuned_r2 = [results[m]['val_r2'] for m in models]
tuned_r2 = [tuned_results[m]['val_r2'] for m in models]
improvement = [tuned_results[m]['improvement'] for m in models]

x = np.arange(len(models))
width = 0.35

fig, axes = plt.subplots(1, 2, figsize=(14,5))

# R¬≤ comparison
axes[0].bar(x - width/2, untuned_r2, width, label='Untuned', alpha=0.7, color='lightblue')
axes[0].bar(x + width/2, tuned_r2, width, label='Tuned', alpha=0.7, color='lightgreen')
axes[0].set_xticks(x)
axes[0].set_xticklabels(models, rotation=45, ha='right')
axes[0].set_ylabel('Validation R¬≤')
axes[0].set_title('Untuned vs Tuned R¬≤ (Higher is Better)')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Improvement plot
colors = ['green' if i>0 else 'red' for i in improvement]
axes[1].bar(models, improvement, color=colors, alpha=0.7)
axes[1].axhline(0, color='black', linestyle='--', alpha=0.8)
axes[1].set_ylabel('R¬≤ Improvement')
axes[1].set_title('Tuning Impact (Green=Better, Red=Worse)')
axes[1].set_xticklabels(models, rotation=45, ha='right')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# ===========================
# 4Ô∏è‚É£ Best tuned model
# ===========================
best_model_name = max(tuned_results, key=lambda m: tuned_results[m]['val_r2'])
print(f"\nüèÜ BEST TUNED MODEL: {best_model_name}")
print(f"üìä Validation R¬≤: {tuned_results[best_model_name]['val_r2']:.4f}")
print(f"üìà Improvement over untuned: +{tuned_results[best_model_name]['improvement']:.4f}")